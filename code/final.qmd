---
title: "final"
format: html
---

Github repo: https://github.com/kathrynk04/ENVS-193DS_spring-2025_final.git

```{r message=FALSE}
library(tidyverse)
library(janitor)
library(here)
library(ggeffects)
library(MuMIn)

sst <- read_csv(
  here("data", "SST_update2023.csv"))
```

## Problem 1: Research writing

# a. Transparent statistical methods

In part 1, they used Pearson's r. In part 2, they used a one-way ANOVA.

# b. More information needed

Another test that should be included is Tukey's HSD in order to determine which source group means are different from each other. A piece of additional information that should be included is the effect size, which is the eta squared, to determine how large of an effect the source had on average nitrogen load.

# c. Suggestions for rewriting

We found a relationship (Pearson's r = correlation coefficient) between distance from headwater and annual total nitrogen load, where distance from headwater significantly predicted annual total nitrogen load (F(degrees of freedom, 1) = F-statistic, p = 0.03, $\alpha$ = significance level). We found a difference between sources in average nitrogen load (one-way ANOvA, F(4, within groups df) = F-statistic, p = 0.02, $\alpha$ = significance level).

## Problem 2: Data visualization

# a. Cleaning and summarizing

```{r message=FALSE}
sst_clean <- sst |> # storing the data into a new cleaned data frame
  clean_names() |> #
  mutate(year = year(date),
         month = month(date)) |>
  filter(year > 2017) |>
  mutate(month = case_match(as.character(month),
                            "1" ~ "Jan",
                            "2" ~ "Feb",
                            "3" ~ "Mar",
                            "4" ~ "Apr",
                            "5" ~ "May",
                            "6" ~ "Jun",
                            "7" ~ "Jul",
                            "8" ~ "Aug",
                            "9" ~ "Sep",
                            "10" ~ "Oct",
                            "11" ~ "Nov",
                            "12" ~ "Dec"),
         month = factor(month,
                        levels = c("Jan",
                                   "Feb",
                                   "Mar",
                                   "Apr",
                                   "May",
                                   "Jun",
                                   "Jul",
                                   "Aug",
                                   "Sep",
                                   "Oct",
                                   "Nov",
                                   "Dec"),
                        ordered = TRUE),
         year = factor(as.character(year),
                       levels = as.character(2018:2023),
                       ordered = FALSE)) |>
  group_by(year, month) |>
  summarize(mean_monthly_sst = mean(temp),
            .groups = "drop") |>
  mutate(mean_monthly_sst = round(mean_monthly_sst, 1)) # Round result

str(sst_clean)

slice_sample(sst_clean, n = 5)
```

# b. Visualize the data

```{r message=FALSE}
ggplot(data = sst_clean,
       aes(x = month,
           y = mean_monthly_sst,
           color = year,
           group = year)) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = c("2018" = "cadetblue1", # assigning different colors to each year
                                "2019" = "cadetblue3",
                                "2020" = "cadetblue",
                                "2021" = "cadetblue4",
                                "2022" = "darkblue",
                                "2023" = "black")) +
  labs(x = "Month", # labeling the x-axis as "Month"
       y = "Mean monthly sea surface temperature (Â°C)") +
  theme_bw() + # making the theme black and white
  theme(panel.grid = element_blank(),
        legend.position = "inside", legend.position.inside =  c(.1, 0.75)) +
  guides(color = guide_legend("Year"))
```
## Problem 3: Data analysis

```{r message=FALSE}
nest_boxes <- read_csv(here("data", "occdist.csv"))
```

```{r}
#| echo: false

nest_boxes_clean <- nest_boxes |>
  clean_names() #|>
#   pivot_longer(cols = c(sp, cs, e, tm),
#                names_to = "species_code",
#                values_to = "presence"
#   ) |> 
#    mutate(species = case_match(species_code,
#                                "sp" ~ "Swift Parrot",
#                                "cs" ~ "Common Starling",
#                                "e"  ~ "Empty",
#                                "tm" ~ "Tree Martin"),
#           species = as_factor(species),
#           species = fct_relevel(species,
#                                 "Empty",
#                                 "Swift Parrot",
#                                 "Common Starling",
#                                 "Tree Martin")) |> 
#   mutate(edge_distance = as.numeric(edge_distance)) |> 
#   # making sure that water_treatment is a factor and ordering levels
#   mutate(season = as_factor(season),
#          season = fct_relevel(season,
#                                        "2016",
#                                        "2019")) |> 
#   # selecting columns
#   select(species, presence, edge_distance, season) |> 
#   group_by(species, season) |>  # or add edge_distance for finer resolution
#   summarise(count = sum(presence), .groups = "drop")
# 
# str(nest_boxes_clean)
```


# a. Response variable

In this dataset, a 1 indicates the presence of one of the three bird species, Common Starling, Swift Parrot, or Tree Martin, in a nest, or if the 1 is under the column labeled "e", then that means the nest was empty.

# b. Purpose of study

The main difference between Swift Parrots and the other two species in the study is that the nests used in the study were intended to be used by Swift Parrots, because they're critically endangered. So, this study was able to determine how nest box distance from the forest edge and year impacted which species nested, therefore helping inform management in order to place nests in places that attract the target species.

# c. Difference in "seasons"

The seasons are the summer in 2016 and 2019, and the difference between these seasons was that the nests were considered new in 2016 and established in 2019.

# d. Table of models

4 models total:

| Model number | Season | Edge Distance | Predictor list              |  
|:------------:|:------:|:-------------:|-----------------------------|  
| 0            |        |               | no predictors (null model)  |
| 1            |    X   |       X       | all predictors (full model) | 
| 2            |        |       X       | edge distance               |   
| 3            |    X   |               | season                      |  

# e. Run the models

```{r message=FALSE}
model0 <- glm(sp ~ 1,
  data = nest_boxes_clean,  # or add edge_distance if grouped that way
  family = "binomial")


# Model 1: all predictors
model1 <- glm(sp ~ edge_distance + season,
  data = nest_boxes_clean,
  family = "binomial"
)

# Model 2: edge_distance only
model2 <- glm(sp ~ edge_distance,
  data = nest_boxes_clean,
  family = "binomial"
)

# Model 3: season only
model3 <- glm(sp ~ season,
  data = nest_boxes_clean,
  family = "binomial"
)
```

# f. Check the diagnostics

## Problem 4: Affective and exploratory eisualizations

# a. Comparing visualizations

My visualizations are different from each other in the way that I've represented my data because my exploratory visualization compares in the form of boxplots the average amount of time I spent studying when I consumed coffee either while studying or not at all, while my affective visualization shows how much time I spent studying each day and how much caffeine I consumed. In my exploratory visualization, you can't tell how much caffeine I consumed each day that I studied, whereas you can tell by the size of the coffee stain in my affective visualization. Additionally, you can't tell when I consumed caffeine relative to studying in my affective visualization, but you can based on the x-axis label under each boxplot in my exploratory visualization.

My visualizations are all similar because they show how much time I spent studying, but that's the only thing they all have in common.

In my exploratory visualization, you can see that I spent more average time studying whenever I consumed caffeine, but it's much harder, or impossible, to see this distinction in my affective visualization. This is due to the fact that my affective visualization doesn't display the average time spent studying when I consumed caffeine and when I didn't, so it's much harder to compare based on the size of the papers and coffee stains in my affective visualization alone.

I got feedback from one of my peers to darken the shading of the coffee stains in my affective visualization, which I did end up doing and only hadn't done yet because my draft was traced in pen instead of pencil and I didn't want to cover the pencil lines, and she also suggested I include an explanation for the size of the pages, which I was able to explain in my artist statement. My instructor suggested defining the proportion of caffeine, which I was also able to define as the proportion of caffeine consumed each day in my artist statement, and that I draw zig-zags instead of straight lines on my paper. However, I wasn't able to do this because some of the sample days I studied for over 300 or 400 minutes, so it would've taken me a really long time to measure out every centimeter of the zig-zags until I reached over 300 centimeters for multiple days, although I do agree it would've looked cool.